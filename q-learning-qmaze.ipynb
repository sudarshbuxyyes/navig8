{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-07T16:37:09.767317Z","iopub.execute_input":"2021-10-07T16:37:09.768424Z","iopub.status.idle":"2021-10-07T16:37:09.809643Z","shell.execute_reply.started":"2021-10-07T16:37:09.768299Z","shell.execute_reply":"2021-10-07T16:37:09.808087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:09.811841Z","iopub.execute_input":"2021-10-07T16:37:09.812176Z","iopub.status.idle":"2021-10-07T16:37:09.820831Z","shell.execute_reply.started":"2021-10-07T16:37:09.812142Z","shell.execute_reply":"2021-10-07T16:37:09.819042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maze = np.array([ [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n    [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n    [ 1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.],\n    [ 1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n    [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.]\n])\n","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:09.822729Z","iopub.execute_input":"2021-10-07T16:37:09.823067Z","iopub.status.idle":"2021-10-07T16:37:09.84586Z","shell.execute_reply.started":"2021-10-07T16:37:09.823016Z","shell.execute_reply":"2021-10-07T16:37:09.845003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maze.size","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:09.848295Z","iopub.execute_input":"2021-10-07T16:37:09.848956Z","iopub.status.idle":"2021-10-07T16:37:09.871642Z","shell.execute_reply.started":"2021-10-07T16:37:09.848926Z","shell.execute_reply":"2021-10-07T16:37:09.871064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visited = 0.8\nagent_mark = 0.5\nLEFT  = 0\nUP = 1\nRIGHT = 2\nDOWN = 3\nactions = {LEFT:'left', UP:'up',RIGHT:'right',DOWN:'down'}\n\nepsilon = 0.1\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:09.87259Z","iopub.execute_input":"2021-10-07T16:37:09.873815Z","iopub.status.idle":"2021-10-07T16:37:09.892894Z","shell.execute_reply.started":"2021-10-07T16:37:09.873753Z","shell.execute_reply":"2021-10-07T16:37:09.892311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Qmaze(object):\n    def __init__(self,maze,agent=(0,0)):\n        self._maze = np.array(maze)\n        nrows, ncols = self._maze.shape\n        self.target = (nrows-1,ncols-1)\n        free_cells = []\n        for r in range(nrows):\n            for c in range(ncols):\n                if(self._maze[r,c]==1.0):\n                    free_cells.append((r,c))\n        self.free_cells = free_cells\n        self.free_cells.remove(self.target) #target cell is not counted as a free cell\n        if(self._maze[self.target]==0.0):\n            raise Exception(\"Target cell cannot be blocked in the maze.\")\n        if not agent in self.free_cells:\n            raise Exception(\"Agent must be in a free cell\")\n        self.reset(agent)\n        \n    def reset(self, agent):\n        self.agent = agent\n        self.maze = np.copy(self._maze)\n        nrows, ncols = self.maze.shape\n        row,col = agent #position of agent\n        self.maze[row,col] = agent_mark\n        self.state = (row,col,'start')\n        self.min_reward = -50\n        self.total_reward = 0\n        self.visited = set()\n    \n    def update_state(self, action):\n        #takes action as input, updates state \n        nrows, ncols = self.maze.shape\n        nrow, ncol, nmode = agent_row, agent_col, mode = self.state\n        if self.maze[agent_row,agent_col]>0.0:\n            self.visited.add((agent_row,agent_col)) #marking visited cell\n            \n        valid_actions = self.valid_actions()\n        if not valid_actions:\n            nmode = 'blocked'\n        elif action in valid_actions:\n            nmode = 'valid'\n            if action == LEFT:\n                ncol-=1\n            if action == RIGHT:\n                ncol+=1\n            if action == UP:\n                nrow-=1\n            if action == DOWN:\n                nrow+=1\n            else:\n                mode = 'invalid'\n        self.state = (nrow,ncol,nmode) #updating the state to new value\n        \n    def get_reward(self):\n        agent_row, agent_col, mode = self.state\n        nrows, ncols = self.maze.shape\n        if (agent_row,agent_col)==self.target:\n            return 1.0\n        if mode=='blocked':\n            return self.min_reward - 1\n        if (agent_row,agent_col) in self.visited:\n            return -0.25\n        if mode == 'invalid':\n            return -0.75\n        if mode == 'valid':\n            return -0.04\n    \n    def act(self,action):\n        self.update_state(action)\n        reward = self.get_reward()\n        self.total_reward += reward\n        status = self.game_status()\n        envstate = self.observe()\n        return envstate, reward, status\n    \n    def observe(self):\n        canvas  = self.draw_env()\n        envstate = canvas.reshape((1,-1))\n        return envstate\n    \n    def draw_env(self):\n        canvas = np.copy(self.maze)\n        nrows,ncols = self.maze.shape\n        #clearing preexisting visual marks\n        for r in range(nrows):\n            for c in range(ncols):\n                if canvas[r,c]>0.0:\n                    canvas[r,c] = 1.0\n        #drawing the agent now.\n        row, col, mode = self.state\n        canvas[row,col] = agent_mark\n        return canvas\n    \n    def game_status(self):\n        if self.total_reward < self.min_reward:\n            return 'lose'\n        agent_row, agent_col, mode = self.state\n        nrows,ncols = self.maze.shape\n        if (agent_row, agent_col) == self.target:\n            return 'win'\n        \n        return 'not over'\n    \n    def valid_actions(self, cell= None):\n        if cell is None:\n            row,col,mode = self.state\n        else:\n            row,col = cell\n        actions = [0,1,2,3]\n        nrows, ncols = self.maze.shape\n        if row == 0:\n            actions.remove(1)\n        elif row == nrows-1:\n            actions.remove(3)\n            \n        if col == 0:\n            actions.remove(0)\n        elif col == ncols-1:\n            actions.remove(2)\n        \n        if row>0 and self.maze[row-1, col] == 0.0:\n            actions.remove(1)\n        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n            actions.remove(3)\n            \n        if col>0 and self.maze[row, col-1] == 0.0:\n            actions.remove(0)\n        if col < ncols -1 and self.maze[row, col +1] == 0.0:\n            actions.remove(2)\n            \n        return actions\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:09.894247Z","iopub.execute_input":"2021-10-07T16:37:09.89471Z","iopub.status.idle":"2021-10-07T16:37:09.935483Z","shell.execute_reply.started":"2021-10-07T16:37:09.894675Z","shell.execute_reply":"2021-10-07T16:37:09.934466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(qmaze):\n    plt.grid('on')\n    nrows, ncols = qmaze.maze.shape\n    ax = plt.gca()\n    ax.set_xticks(np.arange(0.5,nrows,1))\n    ax.set_yticks(np.arange(0.5,nrows,1))\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    canvas = np.copy(qmaze.maze)\n    for row,col in qmaze.visited:\n        canvas[row,col] = 0.6\n    agent_row, agent_col, _ = qmaze.state\n    canvas[agent_row, agent_col] = 0.3   # agent cell\n    canvas[nrows-1, ncols-1] = 0.9 # target cell\n    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:10.025624Z","iopub.execute_input":"2021-10-07T16:37:10.026709Z","iopub.status.idle":"2021-10-07T16:37:10.035342Z","shell.execute_reply.started":"2021-10-07T16:37:10.026637Z","shell.execute_reply":"2021-10-07T16:37:10.033749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qmaze = Qmaze(maze)\ncanvas, reward, game_over = qmaze.act(DOWN)\nprint(\"reward = \", reward)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:10.037592Z","iopub.execute_input":"2021-10-07T16:37:10.037887Z","iopub.status.idle":"2021-10-07T16:37:10.068117Z","shell.execute_reply.started":"2021-10-07T16:37:10.037855Z","shell.execute_reply":"2021-10-07T16:37:10.06573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def play_game(model, qmaze, agent_cell):\n    qmaze.reset(agent_cell)\n    envstate = qmaze.observe()\n    while True:\n        prev_envstate = envstate\n        # get next action\n        q = model.predict(prev_envstate)\n        action = np.argmax(q[0])\n\n        # apply action, get rewards and new state\n        envstate, reward, game_status = qmaze.act(action)\n        if game_status == 'win':\n            return True\n        elif game_status == 'lose':\n            return False\n","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:37:10.070631Z","iopub.execute_input":"2021-10-07T16:37:10.071352Z","iopub.status.idle":"2021-10-07T16:37:10.086697Z","shell.execute_reply.started":"2021-10-07T16:37:10.071321Z","shell.execute_reply":"2021-10-07T16:37:10.085702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def completion_check(model, qmaze):\n    for cell in qmaze.free_cells:\n        if not qmaze.valid_actions(cell):\n            return False\n        if not play_game(model, qmaze, cell):\n            return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2021-10-07T16:59:59.17636Z","iopub.execute_input":"2021-10-07T16:59:59.176731Z","iopub.status.idle":"2021-10-07T16:59:59.204583Z","shell.execute_reply.started":"2021-10-07T16:59:59.176631Z","shell.execute_reply":"2021-10-07T16:59:59.203509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Experience(object):\n    def __init__(self, model, max_memory = 100, discount = 0.95):\n        self.model = model\n        self.max_memory - max_memory\n        self.discount = discount\n        self.memory = list()\n        self.num_actions = model.output_shape[-1]\n    \n    def predict(self,envstate):\n        return self.model.predict(envstate)[0]\n    \n    def get_data(self, data_size = 10):\n        env_size = self.memory[0][0].shape[1]\n        mem_size = len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T17:12:31.384738Z","iopub.execute_input":"2021-10-07T17:12:31.386156Z","iopub.status.idle":"2021-10-07T17:12:31.393889Z","shell.execute_reply.started":"2021-10-07T17:12:31.3861Z","shell.execute_reply":"2021-10-07T17:12:31.392363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}